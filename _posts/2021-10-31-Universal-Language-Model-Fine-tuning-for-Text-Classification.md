---
layout: post
author:
  name: Paper ID 57
  difficulty: Difficulty - Medium
share: true
title: Universal Language Model Fine-tuning for Text Classification
categories:
- NLP
- medium
tags: []
---
**Abstract** - Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code
**Paper** - [https://arxiv.org/abs/1801.06146v5](https://arxiv.org/abs/1801.06146v5)
**Dataset -** [https://ai.stanford.edu/~amaas/data/sentiment/ ; https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://ai.stanford.edu/~amaas/data/sentiment/ ; https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)
    