TA Name,TA BITS Email,Paper Title,Paper Abstract,Paper Link,Difficulty,Tags,Deliverables ,Link to code,Dataset Link
Achleshwar Luthra,Achleshwar L.,Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring,"Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multiscale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.",https://openaccess.thecvf.com/content_cvpr_2017/papers/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.pdf,Medium,"Computer Vision, , Image Enhancement",1. End-to-end model 2. Training on a toy dataset 3. Inference using pretrained network,https://github.com/SeungjunNah/DeepDeblur-PyTorch,https://github.com/shuochsu/DeepVideoDeblurring
Achleshwar Luthra,Achleshwar L.,FFA-Net: Feature Fusion Attention Network for Single Image Dehazing,"In this paper, we propose an end-to-end feature fusion attention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components: 1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attentionbased different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers",https://arxiv.org/abs/1911.07559,Hard,"Computer Vision, Attention, , Image Enhancement",1. End-to-end model 2. Training on a toy dataset 3. Inference using pretrained network,https://github.com/zhilin007/FFA-Net,https://paperswithcode.com/dataset/d-hazy
Achleshwar Luthra,Achleshwar L.,Learning Enriched Features for Real Image Restoration and Enhancement,"With the goal of recovering high-quality image content from its degraded version, image restoration enjoys numerous applications, such as in surveillance, computational photography, medical imaging, and remote sensing. Recently, convolutional neural networks (CNNs) have achieved dramatic improvements over conventional approaches for image restoration task. Existing CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatially precise but contextually less robust results are achieved, while in the latter case, semantically reliable but spatially less accurate outputs are generated. In this paper, we present a novel architecture with the collective goals of maintaining spatially-precise high-resolution representations through the entire network, and receiving strong contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing several key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) spatial and channel attention mechanisms for capturing contextual information, and (d) attention based multi-scale feature aggregation. In the nutshell, our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on five real image benchmark datasets demonstrate that our method, named as MIRNet, achieves state-of-the-art results for a variety of image processing tasks, including image denoising, super-resolution and image enhancement.",https://arxiv.org/abs/2003.06792,Medium,"Computer Vision, Image Enhancement",1. End-to-end model 2. Training on a toy dataset 3. Inference using pretrained network,https://github.com/swz30/MIRNet,https://github.com/shuochsu/DeepVideoDeblurring
Achleshwar Luthra,Achleshwar L.,SDWNet: A Straight Dilated Network with Wavelet Transformation for Image Deblurring,"Image deblurring is a classical computer vision problem that aims to recover a sharp image from a blurred image. To solve this problem, existing methods apply the EncodeDecode architecture to design the complex networks to make a good performance. However, most of these methods use repeated up-sampling and down-sampling structures to expand the receptive field, which results in texture information loss during the sampling process and some of them design the multiple stages that lead to difficulties with convergence. Therefore, our model uses dilated convolution to enable the obtainment of the large receptive field with high spatial resolution. Through making full use of the different receptive fields, our method can achieve better performance. On this basis, we reduce the number of up-sampling and down-sampling and design a simple network structure. Besides, we propose a novel module using the wavelet transform, which effectively helps the network to recover clear high-frequency texture details.",https://arxiv.org/abs/2110.05803,Medium,"Computer Vision, Image Enhancement",1. End-to-end model 2. Training on a toy dataset 3. Inference using pretrained network,https://github.com/FlyEgle/SDWNet,https://github.com/shuochsu/DeepVideoDeblurring
Achleshwar Luthra,Achleshwar L.,YOLOv3: An Incremental Improvement,"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster.",https://arxiv.org/abs/1804.02767,Easy,"Computer Vision, Object Detection",1. End-to-end model 2. Training on a toy dataset 3. Inference using pretrained network,https://www.catalyzex.com/redirect?url=https://pjreddie.com/yolo/,https://www.kaggle.com/datasets/brsdincer/vehicle-detection-image-set
Achleshwar Luthra,Achleshwar L.,UNet++: A Nested U-Net Architecture for Medical Image Segmentation,"In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.",https://arxiv.org/abs/1807.10165,Medium,"Computer Vision, Image Segmentation",1. End-to-end model 2. Training on a toy dataset 3. Inference using pretrained network,https://paperswithcode.com/method/unet,https://paperswithcode.com/task/semantic-segmentation
Achleshwar Luthra,Achleshwar L.,Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs,"We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances of vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields, and higher shape bias rather than texture bias. ",https://arxiv.org/pdf/2203.06717v2.pdf,Medium,"Computer Vision, , Image Classification",1. End-to-end model 2. Training on a toy dataset 3. Inference using pretrained network,https://github.com/DingXiaoH/RepLKNet-pytorch,https://paperswithcode.com/dataset/cifar-100
Devaansh Gupta,Devaansh Gupta,Deep Learning for Extreme Multi-label Text Classification,"Extreme multi-label text classification (XMTC) refers to the prob- lem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. The huge label space raises research challenges such as data sparsity and scalability. Significant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. This paper presents the first attempt at applying deep learning to XMTC, with a family of new Convolutional Neural Net- work (CNN) models which are tailored for multi-label classification in particular. With a comparative evaluation of 7 state-of-the-art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in partic- ular, it outperformed the second best method by 11.7% ∼ 15.3% in precision@K and by 11.5% ∼ 11.7% in NDCG@K for K = 1,3,5.",http://nyc.lti.cs.cmu.edu/yiming/Publications/jliu-sigir17.pdf,Medium,"NLP, Extreme Multilable Classification, Convolutions","1. Model training on EUR-Lex, 2. Reprodcude results",https://github.com/siddsax/XML-CNN/blob/master/code/cnn_train.py,https://drive.google.com/file/d/0B3lPMIHmG6vGU0VTR1pCejFpWjg/view?usp=sharing&resourcekey=0-SurjZ4z_5Tr38jENzf2Iwg
Devaansh Gupta,Devaansh Gupta,LightXML: Transformer with dynamic negative sampling for High-Performance Extreme Multi-label Text Classiﬁcation,"Extreme Multi-label text Classification (XMC) is a task of finding the most relevant labels from a large label set. Nowadays deep learning-based methods have shown significant success in XMC. However, the existing methods (e.g., AttentionXML and X-Transformer etc) still suffer from 1) combining several models to train and predict for one dataset, and 2) sampling negative labels statically during the process of training label ranking model, which reduces both the efficiency and accuracy of the model. To address the above problems, we proposed LightXML, which adopts end-to-end training and dynamic negative labels sampling. In LightXML, we use generative cooperative networks to recall and rank labels, in which label recalling part generates negative and positive labels, and label ranking part distinguishes positive labels from these labels. Through these networks, negative labels are sampled dynamically during label ranking part training by feeding with the same text representation. Extensive experiments show that LightXML outperforms state-of-the-art methods in five extreme multi-label datasets with much smaller model size and lower computational complexity. In particular, on the Amazon dataset with 670K labels, LightXML can reduce the model size up to 72% compared to AttentionXML",https://arxiv.org/pdf/2101.03305,Hard,"NLP, Extreme Multilable Classification, Convolutions","1. Model training on EUR-Lex, 2. Reprodcude results 3. Results with different features from bert 4. Ensembling methods",https://github.com/kongds/LightXML,https://drive.google.com/file/d/0B3lPMIHmG6vGU0VTR1pCejFpWjg/view?usp=sharing&resourcekey=0-SurjZ4z_5Tr38jENzf2Iwg
Devaansh Gupta,Devaansh Gupta,Conditional Convolutions for Instance Segmentation,"We propose a simple yet effective instance segmentation framework, termed CondInst (conditional convolutions for instance segmentation). Top-performing instance segmentation methods such as Mask R-CNN rely on ROI operations (typically ROIPool or ROIAlign) to obtain the final instance masks. In contrast, we propose to solve instance segmentation from a new perspective. Instead of using instance-wise ROIs as inputs to a network of fixed weights, we employ dynamic instance-aware networks, conditioned on instances. CondInst enjoys two advantages: 1) Instance segmentation is solved by a fully convolutional network, eliminating the need for ROI cropping and feature alignment. 2) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference. We demonstrate a simpler instance segmentation method that can achieve improved performance in both accuracy and inference speed. On the COCO dataset, we outperform a few recent methods including well-tuned Mask RCNN baselines, without longer training schedules needed. ",https://arxiv.org/pdf/2003.05664,Hard,"Computer Vision, Instance Segmentation",1. Model Training on COCO Min,https://github.com/aim-uofa/adet,https://github.com/giddyyupp/coco-minitrain
Devaansh Gupta,Devaansh Gupta,CondConv: Conditionally Parameterized Convolutions for Efficient Inference,"Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3% accuracy with only 413M multiply-adds.",https://arxiv.org/pdf/1904.04971,Easy,Convolutions,1. Model Training Time on a standard InceptionNet vs InceptionNet training with CondConv/Replace a convolution in resnet with CondConv and see the results,https://github.com/d-li14/condconv.pytorch,CIFAR-10
Devaansh Gupta,Devaansh Gupta,Superhuman Accuracy on the SNEMI3D Connectomics Challenge,"For the past decade, convolutional networks have been used for 3D reconstruction of neurons from electron microscopic (EM) brain images. Recent years have seen great improvements in accuracy, as evidenced by submissions to the SNEMI3D benchmark challenge. Here we report the first submission to surpass the estimate of human accuracy provided by the SNEMI3D leaderboard. A variant of 3D U-Net is trained on a primary task of predicting affinities between nearest neighbor voxels, and an auxiliary task of predicting long-range affinities. The training data is augmented by simulated image defects. The nearest neighbor affinities are used to create an oversegmentation, and then supervoxels are greedily agglomerated based on mean affinity. The resulting SNEMI3D score exceeds the estimate of human accuracy by a large margin. While one should be cautious about extrapolating from the SNEMI3D benchmark to real-world accuracy of large-scale neural circuit reconstruction, our result inspires optimism that the goal of full automation may be realizable in the future",https://arxiv.org/pdf/1706.00120,Medium,"1. 3D Computer Vision, 2. Medical Imaging",1. Model Training on subset of SNEMI Dataset,https://github.com/zudi-lin/pytorch_connectomics,wget http://hp06.mindhackers.org/rhoana_product/dataset/snemi.zip
Devaansh Gupta,Devaansh Gupta,Distilling the Knowledge in a Neural Network,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",https://arxiv.org/pdf/1503.02531.pdf,Easy,1. NLP 2. Transformer 3. BERT,1. Distillation of a BERT Base into a BERT Small on a simple sentence classification task,https://wandb.ai/cayush/bert-finetuning/reports/Sentence-Classification-With-Huggingface-BERT-and-W-B--Vmlldzo4MDMwNA,
Devaansh Gupta,Devaansh Gupta,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.",https://arxiv.org/pdf/1905.11946v5.pdf,Medium,"1. Computer Vision, 2. Model Scaling","1. Model Training on CIFAR-10, 2. Transfer Learning on ImageNet pretrained on CIFAR-10",https://github.com/lukemelas/EfficientNet-PyTorch,torchvision
Snehil Gupta,Snehil Gupta,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,"Summary: RNN(Seq2Seq) and Transformers based Neural Machine Translation systems for translation of low resource languages such as Cherokee, Abstract: Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world, and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively, and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization. Our data, code, and demo will be publicly available at https://github.com/ZhangShiyue/ChrEn

",https://arxiv.org/abs/2010.04791,Easy,"NLP, Language Translation, Neural Machine Translation(NMT)","1. End-to-End model 2. Training on Dataset. It is an assignment from the cs224n course, so would be pretty easy to follow along",https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/assignments/a4.zip,https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/assignments/a4.zip
Snehil Gupta,Snehil Gupta,A Closer Look at Spatiotemporal Convolutions for Action Recognition,"Summary: The paper discusses several forms of spatiotemporal convolutions for video analysis and studies their effects on action recognition.It empirically demonstrates the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning.The paper also proposes the design of a new spatiotemporal convolution block “R(2+1)D”, which produces results superior/comparable to state of the art on popular action recognition datasets such as : Sports - 1M, Kinetics, UCF 101 and HMDB51.
",https://arxiv.org/abs/1711.11248,Hard,"Computer Vision, Action Recognition",1. End-to-End Model 2. Training on toy dataset 3. Inference on pre-trained network,https://github.com/irhum/R2Plus1D-PyTorch,HMDB 51: https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  MilkBottleToyDataset: https://github.com/microsoft/computervision-recipes/blob/master/scenarios/action_recognition/01_training_introduction.ipynb
Snehil Gupta,Snehil Gupta,"Large-scale weakly-supervised pre-training for video action recognition","Summary: Explores pre-training on large datasets for action recogniton. Mainly focuses on pre-training of R(2+1)D netoworks, Abstract:Current fully-supervised video datasets consist of only a few hundred thousand videos and fewer than a thousand domain-specific labels. This hinders the progress towards advanced video architectures. This paper presents an in-depth study of using large volumes of web videos for pre-training video models for the task of action recognition. Our primary empirical finding is that pre-training at a very large scale (over 65 million videos), despite on noisy social-media videos and hashtags, substantially improves the state-of-the-art on three challenging public action recognition datasets. Further, we examine three questions in the construction of weakly-supervised video action datasets. First, given that actions involve interactions with objects, how should one construct a verb-object pre-training label space to benefit transfer learning the most? Second, frame-based models perform quite well on action recognition; is pre-training for good image features sufficient or is pre-training for spatio-temporal features valuable for optimal transfer learning? Finally, actions are generally less well-localized in long videos vs. short videos; since action labels are provided at a video level, how should one choose video clips for best performance, given some fixed budget of number or minutes of videos?
",https://arxiv.org/abs/1905.00561,Hard,"Computer Vision, Action Recognition",1. End-to-End Model 2. Inference on pre-trained network,https://github.com/irhum/R2Plus1D-PyTorch,https://github.com/microsoft/computervision-recipes/tree/master/scenarios/action_recognition
Snehil Gupta,Snehil Gupta,Attention is all you need,"Summary: This paper was a more advanced step in the use of the attention mechanism being the main basis for a model called Transformer, Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.
",https://arxiv.org/abs/1706.03762,Medium,"NLP, Language Translation, Neural Machine Translation(NMT)",1. End-to-End Model for a simple Eng-Spanish language translation problem.,https://github.com/edumunozsala/Transformer-NMT,http://www.manythings.org/anki/
Snehil Gupta,Snehil Gupta,Siamese LSTM,"Summary: The paper introduces a new model named Manhattan LSTM(MaLSTM) which is a siamese LSTM network that is used for similarity tasks, Abstract: We present a siamese adaptation of the Long Short-Term Memory (LSTM) network for labeled data comprised of pairs of variable-length sequences. Our model is applied to assess semantic similarity between sentences, where we exceed state of the art, outperforming carefully handcrafted features and recently proposed neural network systems of greater complexity. For these applications, we provide word-embedding vectors supplemented with synonymic information to the LSTMs, which use a fixed size vector to encode the underlying meaning expressed in a sentence (irrespective of the particular wording/syntax). By restricting subsequent operations to rely on a simple Manhattan metric, we compel the sentence representations learned by our model to form a highly structured space whose geometry reflects complex semantic relationships. Our results are the latest in a line of findings that showcase LSTMs as powerful language models capable of tasks requiring intricate understanding.

",https://dl.acm.org/doi/10.5555/3016100.3016291,Medium,"NLP, Language Similarity",1. End-to-end Model with training to detect semantic similarity between question pairs.,https://github.com/likejazz/Siamese-LSTM,https://www.kaggle.com/c/quora-question-pairs/data
Snehil Gupta,Snehil Gupta,"A Neural Algorithm of Artistic Style
","Summary: The paper presents an algorithm for combining the content of one image with the style of another image using convolutional neural networks, Abstract: In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.",https://arxiv.org/abs/1508.06576,Easy,"Computer Vision, Neural Style Transfer",1. End-to-end model inference with pre-trained vgg19 network.,https://github.com/animesh-s/Neural-Style-Transfer,NA
Snehil Gupta,Snehil Gupta,"Deep Residual Learning for Image Recognition
","Summary: The paper introduces and explains training of a new class of large networks known as Residual Networks, Abstract: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.",https://arxiv.org/abs/1512.03385,Easy,"Computer Vision, Image Recognition",1. End-to-End model 2. Fine tuning and Inference on pre-trained network for hand-sign detection.,https://github.com/robbertliu/deeplearning.ai-andrewNG/blob/master/COURSE%204%20Convolutional%20Neural%20Networks/Week%2002/ResNets/Residual%20Networks.ipynb,Links available in repo
Shrey Shah,Shrey Shah,"You Only Learn One Representation: Unified Network for Multiple Tasks
                ","People ``understand'' the world via vision, hearing, tactile, and also the past experience. Human experience can be learned through normal learning (we call it explicit knowledge), or subconsciously (we call it implicit knowledge). These experiences learned through normal learning or subconsciously will be encoded and stored in the brain. Using these abundant experience as a huge database, human beings can effectively process data, even they were unseen beforehand. In this paper, we propose a unified network to encode implicit knowledge and explicit knowledge together, just like the human brain can learn knowledge from normal learning as well as subconsciousness learning. The unified network can generate a unified representation to simultaneously serve various tasks. We can perform kernel space alignment, prediction refinement, and multi-task learning in a convolutional neural network. The results demonstrate that when implicit knowledge is introduced into the neural network, it benefits the performance of all tasks. We further analyze the implicit representation learnt from the proposed unified network, and it shows great capability on catching the physical meaning of different tasks. The source code of this work is at : https://github.com/WongKinYiu/yolor.",https://arxiv.org/abs/2105.04206,Medium,Computer Vision; Feature Extracction,"1. End to End model training, and result generation. 2) Generate inference on a toy dataset",https://github.com/WongKinYiu/yolor,https://cocodataset.org/#home
Shrey Shah,Shrey Shah,SOLO: Segmenting Objects by Locations,"We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of ""instance categories"", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.",https://arxiv.org/abs/1912.04488,Easy,Computer Vision; Instance segmentation,"1. End to End model training, and result generation. 2) Generate inference on a toy dataset 3) Plotting visualizations",https://github.com/WXinlong/SOLO,https://cocodataset.org/#home
Shrey Shah,Shrey Shah,FCOS: Fully Convolutional One-Stage Object Detection,"We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1",https://arxiv.org/abs/1904.01355,Medium,Computer Vision; Object Detection,"1. End to End model training, and result generation. 2) Generate inference on a toy dataset",https://github.com/tianzhi0549/FCOS,https://cocodataset.org/#home
Shrey Shah,Shrey Shah,DensePose: Dense Human Pose Estimation In The Wild,"In this work, we establish dense correspondences between RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We first gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efficient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence 'in the wild', namely in the presence of background, occlusions and scale variations. We improve our training set's effectiveness by training an 'inpainting' network that can fill in missing groundtruth values and report clear improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter; we further improve accuracy through cascading, obtaining a system that delivers highly0accurate results in real time. Supplementary materials and videos are provided on the project page http://densepose.org",https://arxiv.org/abs/1802.00434,Easy,Computer Vision; Pose estimation,"1. End to End model training, and result generation. 2) Generate inference on a toy dataset 3) Plotting visualizations",https://github.com/facebookresearch/detectron,
Shrey Shah,Shrey Shah,Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation,"Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.",https://arxiv.org/abs/1806.03185,Medium,CNN; Audio Seperation,1. End to End model training 2. Results on custom chosen songs,https://github.com/f90/Wave-U-Net,https://sigsep.github.io/datasets/musdb.html
Shrey Shah,Shrey Shah,FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking,"Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at https://github.com/ifzhang/FairMOT.",https://arxiv.org/abs/2004.01888,Hard,Computer Vision; Object tracking,"1. End to end model training, 2. Results Visualisation 3. Training on custom dataset",https://github.com/ifzhang/FairMOT,https://cocodataset.org/#home
Shrey Shah,Shrey Shah,CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models,"Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through ""do-operation"" to the causal factors.",https://arxiv.org/abs/2004.08697,Hard,Generative Models,"1. End to End model training, 2. Generation of Images",https://github.com/huawei-noah/trustworthyAI/tree/master/Causal_Disentangled_Representation_Learning,https://www.crowdhuman.org/
Sumantrak Mukherjee,Sumantrak Mukherjee,Causal Discovery with Reinforcement Learning ,"Discovering causal structure among a set of variables is a fundamental problem in
many empirical sciences. Traditional score-based casual discovery methods rely on
various local heuristics to search for a Directed Acyclic Graph (DAG) according to
a predefined score function. While these methods, e.g., greedy equivalence search,
may have attractive results with infinite samples and certain model assumptions,
they are less satisfactory in practice due to finite data and possible violation of
assumptions. Motivated by recent advances in neural combinatorial optimization,
we propose to use Reinforcement Learning (RL) to search for the DAG with
the best scoring. Our encoder-decoder model takes observable data as input and
generates graph adjacency matrices that are used to compute rewards. The reward
incorporates both the predefined score function and two penalty terms for enforcing
acyclicity. In contrast with typical RL applications where the goal is to learn a
policy, we use RL as a search strategy and our final output would be the graph,
among all graphs generated during training, that achieves the best reward. We
conduct experiments on both synthetic and real datasets, and show that the proposed
approach not only has an improved search ability but also allows a flexible score
function under the acyclicity constraint.",https://arxiv.org/pdf/1906.04477.pdf,Hard,"Causality, Reinforcement Learning","1. End to End model training, possibly implement the code on newer datasets",https://github.com/huawei-noah/trustworthyAI,https://github.com/huawei-noah/trustworthyAI
Sumantrak Mukherjee,Sumantrak Mukherjee,Mitgating Unwanted Bias with Adversarial Debiasing ,"Machine learning is a tool for building models that accurately
represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained
models will reflect those biases. We present a framework for
mitigating such biases by including a variable for the group
of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data,
produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected
variable Z, here gender or zip code.
The objective is to maximize the predictors ability to predict
Y while minimizing the adversary’s ability to predict Z. Applied to analogy completion, this method results in accurate
predictions that exhibit less evidence of stereotyping Z. When
applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not
lose much accuracy while achieving very close to equality of
odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range
of gradient-based learning models, including both regression
and classification tasks.",https://arxiv.org/pdf/1801.07593.pdf,Medium,"Data Science, Fairness in ML",1. End to End model training 2. Training on Dataset provided 3. Visualisations with explanations,https://aif360.readthedocs.io/en/latest/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html,https://www.kaggle.com/danofer/compass
Sumantrak Mukherjee,Sumantrak Mukherjee,Deep Reinforcement Learning at the Edge of the Statistical Precipice,"Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field.",https://arxiv.org/abs/2108.13264,Hard,"Statistics, Reinforcement Learning",1. End to End Model Training 2. Try this on some algorithms not already mentioned in the paper,https://github.com/google-research/rliable,
Sumantrak Mukherjee,Sumantrak Mukherjee,Automatic Data Augmentation for Generalization in Deep Reinforcement Learning,"Deep reinforcement learning (RL) agents often fail to generalize to unseen scenarios, even when they are trained on many instances of semantically similar environments. Data augmentation has recently been shown to improve the sample efficiency and generalization of RL agents. However, different tasks tend to benefit from different kinds of data augmentation. In this paper, we compare three approaches for automatically finding an appropriate augmentation. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for certain actor-critic algorithms. We evaluate our methods on the Procgen benchmark which consists of 16 procedurally-generated environments and show that it improves test performance by ~40% relative to standard RL algorithms. Our agent outperforms other baselines specifically designed to improve generalization in RL. In addition, we show that our agent learns policies and representations that are more robust to changes in the environment that do not affect the agent, such as the background.",https://arxiv.org/abs/2006.12862,Medium ,"Reinforcement Learning, Representation ",1. End to End Model Training 2. Visualisation of Data with Explanations,https://github.com/rraileanu/auto-drac,
Sumantrak Mukherjee,Sumantrak Mukherjee,On Fairness and Calibration,"The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be ""fair."" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.",https://arxiv.org/abs/1709.02012,Easy,"Fairness in AI, Data Science",1. End to End Model Training 2. Visualisation ,https://github.com/gpleiss/equalized_odds_and_calibration/blob/master/calib_eq_odds.py,
Sumantrak Mukherjee,Sumantrak Mukherjee,Rainbow: Combining Improvements in Deep Reinforcement Learning,"The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.",https://arxiv.org/abs/1710.02298,Easy,Reinforcement Learning ,1. End to End Model Training on any preferred Gym environment,https://github.com/Kaixhin/Rainbow,
Sumantrak Mukherjee,Sumantrak Mukherjee,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",https://arxiv.org/abs/2010.11929,Medium,"Computer Vision, Transformers",1. End to End Model Training ,https://github.com/google-research/vision_transformer,
Harsh Sulakhe,Harsh Sulakhe,Unpaired Portrait Drawing Generation via Asymmetric Cycle Mapping,"Portrait drawing is a common form of art with high abstraction and expressiveness. Due to its unique characteristics, existing methods achieve decent results only with paired training data, which is costly and time-consuming to obtain. In this paper, we address the problem of automatic transfer from face photos to portrait drawings with unpaired  training data. We observe that due to the significant imbalance of information richness between photos and drawings, existing unpaired transfer methods such as CycleGAN tend to embed invisible reconstruction information indiscriminately in the whole drawings, leading to important facial features partially missing in drawings. To address this problem, we propose a novel asymmetric cycle mapping that enforces the reconstruction information to be visible (by a truncation loss) and only embedded in selective facial regions (by a relaxed forward cycle-consistency loss). Along with localized discriminators for the eyes, nose and lips, our method well preserves all important facial features in the generated portrait drawings. By introducing a style classifier and taking the style vector into account, our method can learn to generate portrait drawings in multiple styles using a single network. Extensive experiments show that our model outperforms state-of-the-art methods.
",https://openaccess.thecvf.com/content_CVPR_2020/papers/Yi_Unpaired_Portrait_Drawing_Generation_via_Asymmetric_Cycle_Mapping_CVPR_2020_paper.pdf,Hard,"Computer Vision, Image-to-Image Translation, GANs,",1. End to End Model Training 2. Test functionality of a new loss function (either replacing or adding to the existing ones). 3. Prepare ablation study using this loss function.,https://github.com/yiranran/Unpaired-Portrait-Drawing,Links available in repo
Harsh Sulakhe,Harsh Sulakhe,Perceptual Losses for Real-Time Style Transfer and Super-Resolution,"We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",https://arxiv.org/pdf/1603.08155.pdf,Medium,"Computer Vision, Style Transfer","1. Experiment and show results with networks other than VGG. 2. Experiment and show results using different combinations of layers in VGG itself. 3. Finetune a pre-trained VGG network on a small dataset of your choice, compare results of style transfer",https://github.com/pytorch/tutorials/blob/master/advanced_source/neural_style_tutorial.py,Choice
Harsh Sulakhe,Harsh Sulakhe,Contrastive Learning for Unpaired Image-to-Image Translation,"In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so – maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each “domain” is only a single image.",https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540307.pdf,Medium,"Computer Vision, Image-to-Image Translation, GANs",1. End to End Model Training on a dataset of choice. 2. Experiment with multiple values of loss weights and show results of each.,https://github.com/taesungp/contrastive-unpaired-translation,Links available in repo
Harsh Sulakhe,Harsh Sulakhe,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,"Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",https://arxiv.org/pdf/1612.00593.pdf,Easy,Point Cloud Classification and Segmentation,1. End to End model training 2. Training on a toy dataset 3. Inference using pretrained network,https://github.com/fxia22/pointnet.pytorch,Links available in repo
Harsh Sulakhe,Harsh Sulakhe,StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks,"Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images",https://arxiv.org/pdf/1710.10916v3.pdf,Medium,"GANs, Text-to-Image Synthesis",1. End to End model training 2. Inference using pretrained network,https://github.com/hanzhanggit/StackGAN-v2,Links available in repo
Harsh Sulakhe,Harsh Sulakhe,3D Reconstruction of Novel Object Shapes from Single Images,"Accurately predicting the 3D shape of any arbitrary object in any pose from a single image is a key goal of computer vision research. This is challenging as it requires a model to learn a representation that can infer both the visible and occluded portions of any object using a limited training set. A training set that covers all possible object shapes is inherently infeasible. Such learning-based approaches are inherently vulnerable to overfitting, and successfully implementing them is a function of both the architecture design and the training approach. We present an extensive investigation of factors specific to architecture design, training, experiment design, and evaluation that influence reconstruction performance and measurement. We show that our proposed SDFNet achieves state-of-the-art performance on seen and unseen shapes relative to existing methods GenRe[56] and OccNet[30]. We provide the first large-scale evaluation of single image shape reconstruction to unseen objects. The source code, data and trained models can be found on https://github.com/rehg-lab/3DShapeGen",https://arxiv.org/pdf/2006.07752.pdf,Hard,Single Image 3D Reconstruction,1. End to End model training 2. Inference and visualization using pretrained network,https://github.com/rehg-lab/3DShapeGen,Links available in repo
Harsh Sulakhe,Harsh Sulakhe,Video-to-Video Synthesis,"We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on  a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website.",https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf,Easy,Video Synthesis,1. End to End model training 2. Inference and visualization using pretrained network,https://github.com/NVIDIA/vid2vid,Links available in repo
Kanishk Sharma,Kanishk Sharma,"SING: Symbol-to-Instrument Neural Generator","Recent progress in deep learning for audio synthesis opens the way to models that
directly produce the waveform, shifting away from the traditional paradigm of
relying on vocoders or MIDI synthesizers for speech or music generation. Despite
their successes, current state-of-the-art neural audio synthesizers such as WaveNet
and SampleRNN [24, 17] suffer from prohibitive training and inference times
because they are based on autoregressive models that generate audio samples one
at a time at a rate of 16kHz. In this work, we study the more computationally
efficient alternative of generating the waveform frame-by-frame with large strides.
We present SING, a lightweight neural audio synthesizer for the original task of
generating musical notes given desired instrument, pitch and velocity. Our model
is trained end-to-end to generate notes from nearly 1000 instruments with a single
decoder, thanks to a new loss function that minimizes the distances between the
log spectrograms of the generated and target waveforms. On the generalization
task of synthesizing notes for pairs of pitch and instrument not seen during training,
SING produces audio with significantly improved perceptual quality compared to a
state-of-the-art autoencoder based on WaveNet [4] as measured by a Mean Opinion
Score (MOS), and is about 32 times faster for training and 2, 500 times faster for
inference",https://scontent-bom1-1.xx.fbcdn.net/v/t39.8562-6/240834622_997590737739307_5513666903256671510_n.pdf?_nc_cat=101&ccb=1-5&_nc_sid=ad8a9d&_nc_ohc=OXt8KnyuLlAAX8YgDnG&_nc_ht=scontent-bom1-1.xx&oh=00_AT_m4Ca1BlR5HmgMKdL-LZzYwzDGeBTnmGph7HX4uFwTsw&oe=6241EC7B,Medium,"Audio Synthesis, Autoencoders, Music Generation",1. End to End model training 2. Inference run,https://github.com/facebookresearch/SING,https://magenta.tensorflow.org/datasets/nsynth
Kanishk Sharma,Kanishk Sharma,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset","The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.
We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101.",https://arxiv.org/abs/1705.07750,Medium,"Computer Vision, Action Recognition, Transfer Learning",1. End to End model training 2. Training on a toy dataset 3. Inference using pretrained network,https://github.com/deepmind/kinetics-i3d,https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/
Kanishk Sharma,Kanishk Sharma,Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation,"Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.",https://arxiv.org/pdf/1806.03185v1.pdf,Medium,"U-net, Audio separation",1. End to end model training,https://github.com/f90/Wave-U-Net,http://sisec.inria.fr/2018-professionally-produced-music-recordings/
Kanishk Sharma,Kanishk Sharma,Image-to-Image Translation with Conditional Adversarial Networks,"We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",https://arxiv.org/pdf/1611.07004v3.pdf,Medium,"Computer Vision, GANs, Image to Image Translation",1. End to End model Training,https://github.com/phillipi/pix2pix,https://cmp.felk.cvut.cz/~tylecr1/facade/
Kanishk Sharma,Kanishk Sharma,V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",https://arxiv.org/abs/1606.04797,Easy,Computer Vision,1. End to End model Training,https://github.com/mattmacy/vnet.pytorch,https://luna16.grand-challenge.org/data/
Kanishk Sharma,Kanishk Sharma,Noise2Noise: Learning Image Restoration without Clean Data,"We apply basic statistical reasoning to signal reconstruction by machine learning – learning to map corrupted observations to clean signals – with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans – all corrupted by different processes – based on noisy data only",https://arxiv.org/pdf/1803.04189v3.pdf,Medium,Computer Vision,1. End to End model training 2. Inference and visualization using pretrained network,https://github.com/joeylitalien/noise2noise-pytorch,wget http://images.cocodataset.org/zips/val2017.zip
Kanishk Sharma,Kanishk Sharma,"Deep Photo Style Transfer","This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.",https://arxiv.org/abs/1703.07511,Medium,"Computer Vision, GANs",1. End to End model Training,https://github.com/LouieYang/deep-photo-styletransfer-tf,https://github.com/luanfujun/deep-photo-styletransfer/tree/master/examples
Preetika Verma,Preetika Verma,U-Net - Convolutional Networks for Biomedical Image Segmentation,"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU.",https://arxiv.org/abs/1505.04597,Easy,CV,End to end model training,https://github.com/zhixuhao/unet,http://brainiac2.mit.edu/isbi_challenge/home
Preetika Verma,Preetika Verma,Sentence-BERT-Sentence Embeddings using Siamese BERT-Networks,"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",https://arxiv.org/abs/1908.10084,Easy,NLP,End to end model training,https://www.sbert.net/,https://nlp.stanford.edu/projects/snli/
Preetika Verma,Preetika Verma,Linknet: Exploiting Encoder Representations for Efficient Semantic Segmentation,"Pixel-wise semantic segmentation for visual scene
understanding not only needs to be accurate, but also efficient
in order to find any use in real-time application. Existing
algorithms even though are accurate but they do not focus
on utilizing the parameters of neural network efficiently. As
a result they are huge in terms of parameters and number
of operations; hence slow too. In this paper, we propose a
novel deep neural network architecture which allows it to learn
without any significant increase in number of parameters. Our
network uses only 11.5 million parameters and 21.2 GFLOPs
for processing an image of resolution 3 × 640 × 360. It gives
state-of-the-art performance on CamVid and comparable results
on Cityscapes dataset. We also compare our networks processing
time on NVIDIA GPU and embedded system device with existing
state-of-the-art architectures for different image resolutions.",https://arxiv.org/abs/1707.03718,Hard,CV,End to end model training,https://github.com/davidtvs/Keras-LinkNet,Link available in repo
Preetika Verma,Preetika Verma,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,"Image-to-image translation is a class of vision and
graphics problems where the goal is to learn the mapping
between an input image and an output image using a training set of aligned image pairs. However, for many tasks,
paired training data will not be available. We present an
approach for learning to translate an image from a source
domain X to a target domain Y in the absence of paired
examples. Our goal is to learn a mapping G : X → Y
such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss.
Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a
cycle consistency loss to enforce F(G(X)) ≈ X (and vice
versa). Qualitative results are presented on several tasks
where paired training data does not exist, including collection style transfer, object transfiguration, season transfer,
photo enhancement, etc. Quantitative comparisons against
several prior methods demonstrate the superiority of our
approach.",https://arxiv.org/pdf/1703.10593.pdf,Hard,CV,End to end model training,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix,
Preetika Verma,Preetika Verma,Siamese Networks for One Shot Recognition,"The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.",https://www.cs.cmu.edu//~rsalakhu/papers/oneshot1.pdf,Easy,CV,End to end training,https://github.com/tensorfreitas/Siamese-Networks-for-One-Shot-Learning,https://github.com/brendenlake/omniglot
Preetika Verma,Preetika Verma,Character level convolutional networks for text classification,"This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",https://arxiv.org/abs/1509.01626,Easy,NLP,End to end training,https://github.com/zhangxiangxiao/Crepe,https://drive.google.com/drive/folders/1vZ1agGTdHJDX455Vnl7Y1TW9eXqhUhWx?usp=sharing
Preetika Verma,Preetika Verma,Attention U-Net: Learning Where to Look for the Pancreas,"We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models
trained with AGs implicitly learn to suppress irrelevant regions in an input image
while highlighting salient features useful for a specific task. This enables us to
eliminate the necessity of using explicit external tissue/organ localisation modules
of cascaded convolutional neural networks (CNNs). AGs can be easily integrated
into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy.
The proposed Attention U-Net architecture is evaluated on two large CT abdominal
datasets for multi-class image segmentation. Experimental results show that AGs
consistently improve the prediction performance of U-Net across different datasets
and training sizes while preserving computational efficiency. The source code for
the proposed architecture is publicly available.",https://arxiv.org/pdf/1804.03999.pdf,Easy ,CV,End to end training,https://github.com/sfczekalski/attention_unet,
Sarthak Gupta,Sarthak Gupta,"``Liar, Liar Pants on Fire'': A New Benchmark Dataset for Fake News Detection","Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.",https://aclanthology.org/P17-2067.pdf,Easy,NLP,End to end modelling,https://github.com/UsamaI000/Fake-News-Detection-Pytorch,https://www.cs.ucsb.edu/˜william/ data/liar_dataset.zip
Sarthak Gupta,Sarthak Gupta,Automatic Recognition of Student Engagement using Deep Learning and Facial Expression,"Engagement is a key indicator of the quality of learning experience, and one that plays a major role in developing intelligent educational interfaces. Any such interface requires the ability to recognise the level of engagement in order to respond appropriately; however, there is very little existing data to learn from, and new data is expensive and difficult to acquire. This paper presents a deep learning model to improve engagement recognition from images that overcomes the data sparsity challenge by pre-training on readily available basic facial expression data, before training on specialised engagement data. In the first of two steps, a facial expression recognition model is trained to provide a rich face representation using deep learning. In the second step, we use the model's weights to initialize our deep learning based model to recognize engagement; we term this the engagement model. We train the model on our new engagement recognition dataset with 4627 engaged and disengaged samples. We find that the engagement model outperforms effective deep learning architectures that we apply for the first time to engagement recognition, as well as approaches using histogram of oriented gradients and support vector machines.",https://arxiv.org/abs/1808.02324,Hard,CNN,End to end modelling,https://github.com/omidmnezami/Engagement-Recognition,Link available in repo
Sarthak Gupta,Sarthak Gupta,Sequence to Sequence Learning with Neural Networks,,https://arxiv.org/abs/1409.3215,Hard,NLP,End to end modelling,https://github.com/bentrevett/pytorch-seq2seq,
Sarthak Gupta,Sarthak Gupta,Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps,"Texts appearing in daily scenes that can be recognized by OCR (Optical Character Recognition) tools contain significant information, such as street name, product brand and prices. Two tasks -- text-based visual question answering and text-based image captioning, with a text extension from existing vision-language applications, are catching on rapidly. To address these problems, many sophisticated multi-modality encoding frameworks (such as heterogeneous graph structure) are being used. In this paper, we argue that a simple attention mechanism can do the same or even better job without any bells and whistles. Under this mechanism, we simply split OCR token features into separate visual- and linguistic-attention branches, and send them to a popular Transformer decoder to generate answers or captions. Surprisingly, we find this simple baseline model is rather strong -- it consistently outperforms state-of-the-art (SOTA) models on two popular benchmarks, TextVQA and all three tasks of ST-VQA, although these SOTA models use far more complex encoding mechanisms. Transferring it to text-based image captioning, we also surpass the TextCaps Challenge 2020 winner. We wish this work to set the new baseline for this two OCR text related applications and to inspire new thinking of multi-modality encoder design",https://arxiv.org/abs/2012.05153,Hard,NLP,End to end modelling,https://github.com/ZephyrZhuQi/ssbaseline?utm_source=catalyzex.com,
Sarthak Gupta,Sarthak Gupta,Text Summarization with Pretrained Encoders,"Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.",https://arxiv.org/abs/1908.08345,Medium,NLP ,End to end modelling ,https://github.com/nlpyang/PreSumm,
Jash Shah,JASH SHAH,Node2vec: Scalable Feature Learning for Networks,"Prediction tasks over nodes and edges in networks require careful
effort in engineering features used by learning algorithms. Recent
research in the broader field of representation learning has led to
significant progress in automating prediction by learning the features themselves. However, present feature learning approaches
are not expressive enough to capture the diversity of connectivity
patterns observed in networks.
Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In
node2vec, we learn a mapping of nodes to a low-dimensional space
of features that maximizes the likelihood of preserving network
neighborhoods of nodes. We define a flexible notion of a node’s
network neighborhood and design a biased random walk procedure,
which efficiently explores diverse neighborhoods. Our algorithm
generalizes prior work which is based on rigid notions of network
neighborhoods, and we argue that the added flexibility in exploring
neighborhoods is the key to learning richer representations.
We demonstrate the efficacy of node2vec over existing state-ofthe-art techniques on multi-label classification and link prediction
in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning stateof-the-art task-independent representations in complex networks",https://arxiv.org/pdf/1607.00653.pdf,Hard,Graph-Learning,End to end modelling and implementation on toy dataset to get similar accuracies,https://github.com/aditya-grover/node2vec,https://github.com/benedekrozemberczki/datasets
Jash Shah,JASH SHAH,"Generative Adversarial Network based Heuristics
for Sampling-based Path Planning","—Sampling-based path planning is a popular methodology for robot path planning. With a uniform sampling strategy
to explore the state space, a feasible path can be found without the
complex geometric modeling of the configuration space. However,
the quality of initial solution is not guaranteed and the convergence speed to the optimal solution is slow. In this paper, we
present a novel image-based path planning algorithm to overcome
these limitations. Specifically, a generative adversarial network
(GAN) is designed to take the environment map (denoted as
RGB image) as the input without other preprocessing works. The
output is also an RGB image where the promising region (where a
feasible path probably exists) is segmented. This promising region
is utilized as a heuristic to achieve non-uniform sampling for the
path planner. We conduct a number of simulation experiments to
validate the effectiveness of the proposed method, and the results
demonstrate that our method performs much better in terms
of the quality of initial solution and the convergence speed to
the optimal solution. Furthermore, apart from the environments
similar to the training set, our method also works well on the
environments which are very different from the training set.",https://arxiv.org/pdf/2012.03490.pdf,Medium,"Path Planning, Conditional GANs","Implement RRT to generate dataset, implement the GAN model to predict ROIs, Show the results using Hueristic GANs(optional task)",https://github.com/akanametov/pathgan,Link Available in Repo
Jash Shah,JASH SHAH,"U-Time: A Fully Convolutional Network for Time
Series Segmentation Applied to Sleep Staging","Neural networks are becoming more and more popular for the analysis of physiological time-series. The most successful deep learning systems in this domain combine convolutional and recurrent layers to extract useful features to model temporal relations. Unfortunately, these recurrent models are difficult to tune and optimize. In our experience, they often require task-specific modifications, which makes them challenging to use for non-experts. We propose U-Time, a fully feed-forward deep learning approach to physiological time series segmentation developed for the analysis of sleep data. U-Time is a temporal fully convolutional network based on the U-Net architecture that was originally proposed for image segmentation. U-Time maps sequential inputs of arbitrary length to sequences of class labels on a freely chosen temporal scale. This is done by implicitly classifying every individual time-point of the input signal and aggregating these classifications over fixed intervals to form the final predictions. We evaluated U-Time for sleep stage classification on a large collection of sleep electroencephalography (EEG) datasets. In all cases, we found that U-Time reaches or outperforms current state-ofthe-art deep learning models while being much more robust in the training process and without requiring architecture or hyperparameter adaptation across tasks.",https://proceedings.neurips.cc/paper/2019/file/57bafb2c2dfeefba931bb03a835b1fa9-Paper.pdf,Medium,"Time Series, U-Net",End to end modelling,https://github.com/perslev/U-Time/tree/utime-latest,https://www.physionet.org/content/sleep-edfx/1.0.0/
Jash Shah,JASH SHAH,"DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent
Convolutional Neural Networks","This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs) 1 . Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-ofthe-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.",https://arxiv.org/pdf/1709.08429v1.pdf,Easy,"Visual Odometry, Robotics",End to end modelling,https://github.com/fshamshirdar/DeepVO,Link Available in Repo
Jash Shah,JASH SHAH,"MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER","Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT significantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters",https://arxiv.org/pdf/2110.02178.pdf,Easy,"CV, Transformer, Edge Devices",End to end modelling for image processing task,https://keras.io/examples/vision/mobilevit/,https://www.image-net.org/
Jash Shah,JASH SHAH,Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,"Deep learning (DL) based semantic segmentation methods have been providing state-of-the-art performance in the last few years. More specifically, these techniques have been successfully applied to medical image classification, segmentation, and detection tasks. One deep learning technique, U-Net, has become one of the most popular for these applications. In this paper, we propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net models, which are named RU-Net and R2U-Net respectively. The proposed models utilize the power of U-Net, Residual Network, as well as RCNN. There are several advantages of these proposed architectures for segmentation tasks. First, a residual unit helps when training deep architecture. Second, feature accumulation with recurrent residual convolutional layers ensures better feature representation for segmentation tasks. Third, it allows us to design better U-Net architecture with same number of network parameters with better performance for medical image segmentation. The proposed models are tested on three benchmark datasets such as blood vessel segmentation in retina images, skin cancer segmentation, and lung lesion segmentation. The experimental results show superior performance on segmentation tasks compared to equivalent models including UNet and residual U-Net (ResU-Net).",https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf,Easy,"U-Net, CV",End to end modelling and possibly some updates in Reccurent layers,https://github.com/LeeJunHyun/Image_Segmentation,https://challenge2018.isic-archive.com/task1/training/
Jash Shah,JASH SHAH,Mask R-CNN,"We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition",https://arxiv.org/pdf/1703.06870.pdf,Easy,"Computer Vision, Object Instance Segmentation, Pose Estimation",End to end modelling,https://github.com/facebookresearch/Detectron,Link Available in Repo
Siddharth Awasthi,Siddharth Awasthi,"Auto-Encoding Variational Bayes (VAE)
","a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case",https://arxiv.org/abs/1312.6114v10,Hard,"Auto Encoders, CV","End-to-End model implementation, training on a toy dataset",https://github.com/AntixK/PyTorch-VAE/blob/8700d245a9735640dda458db4cf40708caf2e77f/models/vanilla_vae.py#L8,http://yann.lecun.com/exdb/mnist/
Siddharth Awasthi,Siddharth Awasthi,Seg-Net-A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,"We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.",https://arxiv.org/pdf/1511.00561v3.pdf,Medium,"CV, semantic segmentation","End-to-End model implementation, training on a toy dataset",https://github.com/yassouali/pytorch-segmentation,details available in the paper
Siddharth Awasthi,Siddharth Awasthi,Learning to See in the Dark,"Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learningbased pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fullyconvolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work.",https://arxiv.org/pdf/1805.01934v1.pdf,Medium,"CV,","End-to-End model implementation, training on a toy dataset",https://github.com/cydonia999/Learning_to_See_in_the_Dark_PyTorch,https://github.com/cydonia999/Learning_to_See_in_the_Dark_PyTorch
Siddharth Awasthi,Siddharth Awasthi,3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation,"This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated
setup, the user annotates some slices in the volume to be segmented.
The network learns from these sparse annotations and provides a dense
3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data
set, the network densely segments new volumetric images. The proposed
network extends the previous u-net architecture from Ronneberger et
al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data
augmentation during training. It is trained end-to-end from scratch, i.e.,
no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus
kidney, and achieve good results for both use cases",https://github.com/wolny/pytorch-3dunet/tree/master/pytorch3dunet,medium,3D segmentation,"End-to-End model implementation, training on a toy dataset",https://github.com/wolny/pytorch-3dunet,https://github.com/wolny/pytorch-3dunet/tree/master/resources
Siddharth Awasthi,Siddharth Awasthi,Enhanced Deep Residual Networks for Single Image Super-Resolution,"Recent research on super-resolution has progressed with
the development of deep convolutional neural networks
(DCNN). In particular, residual learning techniques exhibit
improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model
is due to optimization by removing unnecessary modules in
conventional residual networks. The performance is further
improved by expanding the model size while we stabilize
the training procedure. We also propose a new multi-scale
deep super-resolution system (MDSR) and training method,
which can reconstruct high-resolution images of different
upscaling factors in a single model. The proposed methods
show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge",,easy,Super resolution,"End-to-End model implementation, training on a toy dataset",https://github.com/sanghyun-son/EDSR-PyTorch,https://github.com/sanghyun-son/EDSR-PyTorch/tree/master/src/data
Siddharth Awasthi,Siddharth Awasthi,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,"The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18× faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.",https://arxiv.org/pdf/1606.02147v1.pdf,easy,Semantic Segmentation ,"End-to-End model implementation, training on a toy dataset",https://github.com/yassouali/pytorch-segmentation,https://github.com/yassouali/pytorch-segmentation
Siddharth Awasthi,Siddharth Awasthi,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile,"We introduce an extremely computation-efficient CNN
architecture named ShuffleNet, which is designed specially
for mobile devices with very limited computing power (e.g.,
10-150 MFLOPs). The new architecture utilizes two new
operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining
accuracy. Experiments on ImageNet classification and MS
COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1
error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of
40 MFLOPs. On an ARM-based mobile device, ShuffleNet
achieves ∼13× actual speedup over AlexNet while maintaining comparable accuracy.",https://arxiv.org/pdf/1707.01083v2.pdf,easy,classification,"End-to-End model implementation, training on a toy dataset",https://github.com/Mayurji/Image-Classification-PyTorch,https://github.com/Mayurji/Image-Classification-PyTorch
Palaash Agrawal,Palaash Agrawal,Universal Language Model Fine-tuning for Text Classification,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.",,Medium,Self Supervised Learning,Modeing this approach for a computer vision task (this paper is primarily for language tasks),Transfer learning in text | fastai,Any Image classification dataset of choice.
Palaash Agrawal,Palaash Agrawal,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,"mage-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X→Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F:Y→X and introduce a cycle consistency loss to push F(G(X))≈X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",[1703.10593] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (arxiv.org),Medium,Cycle GAN,End to End model implementation,junyanz/pytorch-CycleGAN-and-pix2pix: Image-to-Image Translation in PyTorch (github.com),Index of /~taesung_park/CycleGAN/datasets (berkeley.edu)
Palaash Agrawal,Palaash Agrawal,Cycle Text-to-Image GAN with BERT,"We explore novel approaches to the task of image generation from their respective captions, building on state-of-the-art GAN architectures. Particularly, we baseline our models with the Attention-based GANs that learn attention mappings from words to image features. To better capture the features of the descriptions, we then built a novel cyclic design that learns an inverse function to maps the image back to original caption. Additionally, we incorporated recently developed BERT pretrained word embeddings as our initial text featurizer and observe a noticeable improvement in qualitative and quantitative performance compared to the Attention GAN baseline.",https://arxiv.org/pdf/2003.12137v1.pdf,Medium,"CV, NLP, RNN",1) Implement AttnGAN and CycleGAN 2) Try and obtain the results in fig 5 and fig 6 3) Experiment with different BERT variants (https://huggingface.co/transformers/pretrained_models.html) and observe the results 4) Show the results on 3-4 examples 5) https://github.com/suetAndTie/cycle-image-gan,suetAndTie/cycle-image-gan (github.com),birds.zip - Google Drive
Palaash Agrawal,Palaash Agrawal,Deep Pyramid Convolutional Neural Networks for Text Categorization,"This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.",https://aclanthology.org/P17-1052.pdf,Medium,NLP,End to end implementation,Cheneng/DPCNN: Deep Pyramid Convolutional Neural Networks for Text Categorization in PyTorch (github.com),
Palaash Agrawal,Palaash Agrawal,ViViT: A Video Vision Transformer,"We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we will release code and models.",ViViT: A Video Vision Transformer | IEEE Conference Publication | IEEE Xplore,Hard,CV,"1) Implement any of the Model 2, Model 3 and Model 4 2) Train and Test it on the HMDB51 dataset",rishikksh20/ViViT-pytorch: Implementation of ViViT: A Video Vision Transformer (github.com),http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar
Palaash Agrawal,Palaash Agrawal,Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge,"This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.",[1708.02711] Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge (arxiv.org),Hard,"CV, NLP  ",1) Implement the architecture (using any Python framework) as shown in Fig 1.1 2) Train the model on Visual Genome Dataset 3) Try and replicate the results of the first 3 rows of Table 1 and all of Table 2 ,https://github.com/hengyuan-hu/bottom-up-attention-vqa,bottom-up-attention-vqa/download.sh at master · hengyuan-hu/bottom-up-attention-vqa (github.com)
Palaash Agrawal,Palaash Agrawal,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the cor- responding words in the output sequence. We validate the use of attention with state-of-the- art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",https://arxiv.org/abs/1502.03044,Easy,CV,"1) Train and evaluate soft attention variant model on Flickr8k dataset (Evaluation implies all 4 BLEU scores and METEOR Score on the test set as mentioned in the paper) 2) Change architecture: Use a different CNN architecture than VGG or any used in the paper for the encoder. Train and evaluate on test set, same as before. 3) Form 5 input images and write ground truth captions for them by yourself. Then generate captions using both above trained models on these 10 self-input images, and calculate BLEU and METEOR scores. 4) Visualize attention at each timestep of generated caption for these 5 images","sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning: Show, Attend, and Tell | a PyTorch Tutorial to Image Captioning (github.com)",https://www.kaggle.com/adityajn105/flickr8k/activity